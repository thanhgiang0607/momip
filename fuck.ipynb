{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac7e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2080ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY SPEED (DS) ===\n",
      "DS trained ‚Äî MAE: 2.459h, R2: 0.520\n",
      "Saved DS artifacts: model_DS.pkl, scaler_DS.pkl, le_sender.pkl, le_receiver.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY SPEED (DS) ===\")\n",
    "df_ds = pd.read_csv('delivery speed - uds-orders-aug2024.csv')\n",
    "df_ds = df_ds.dropna(subset = ['deliveredAt', 'createdAt'])\n",
    "df_ds['deliveredAt'] = pd.to_datetime(df_ds['deliveredAt'], errors = 'coerce')\n",
    "df_ds['createdAt'] = pd.to_datetime(df_ds['createdAt'], errors = 'coerce')\n",
    "df_ds = df_ds.dropna(subset=['createdAt', 'deliveredAt'])\n",
    "df_ds['delivery_duration_hours'] = (df_ds['deliveredAt'] - df_ds['createdAt']).dt.total_seconds() / 3600\n",
    "df_ds = df_ds[df_ds['delivery_duration_hours'].between(0.1, 72)]\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
    "    return 2 * R * atan2(sqrt(a), sqrt(1 - a))\n",
    "def extract_district(address):\n",
    "    if pd.isna(address):\n",
    "        return 'unknown'\n",
    "    try: \n",
    "        match = re.search(r\"(Qu·∫≠n|Q\\.?|Huy·ªán|H\\.?)\\s*([\\w\\s\\dƒêƒë√Ç√¢ƒÇƒÉ√ä√™√î√¥∆†∆°∆Ø∆∞]+)\", str(address))\n",
    "        if match:\n",
    "            return match.group(0).strip()\n",
    "    except re.error:\n",
    "        return \"Unknown\"\n",
    "    return \"Unknown\"\n",
    "if 'senderAddress' in df_ds.columns and 'receiverAddress' in df_ds.columns:\n",
    "    df_ds[\"sender_district\"] = df_ds['senderAddress'].apply(extract_district)\n",
    "    df_ds['receiver_district'] = df_ds['receiverAddress'].apply(extract_district)\n",
    "else:\n",
    "    df_ds['sender_district'] = \"Unknown\"\n",
    "    df_ds['receiver_district'] = \"Unknown\"\n",
    "df_ds['hour_of_day'] = df_ds['createdAt'].dt.hour\n",
    "df_ds['day_of_week'] = df_ds['createdAt'].dt.dayofweek\n",
    "df_ds['is_weekend'] = df_ds['day_of_week'].isin([5,6]).astype(int)\n",
    "# encoders for district\n",
    "le_sender = LabelEncoder()\n",
    "le_receiver = LabelEncoder()\n",
    "df_ds['sender_district'] = le_sender.fit_transform(df_ds['sender_district'].astype(str))\n",
    "df_ds['receiver_district'] = le_receiver.fit_transform(df_ds['receiver_district'].astype(str))  # FIXED: use le_receiver here\n",
    "features_ds = ['shippingDistance', 'hour_of_day', 'day_of_week', 'sender_district', 'receiver_district', 'is_weekend']\n",
    "X_ds = df_ds[features_ds].copy()\n",
    "y_ds = df_ds['delivery_duration_hours']\n",
    "scaler_ds = StandardScaler()\n",
    "X_ds_scaled = scaler_ds.fit_transform(X_ds)\n",
    "\n",
    "X_train_ds, X_test_ds, y_train_ds, y_test_ds = train_test_split(X_ds_scaled, y_ds, test_size=0.2, random_state=42)\n",
    "model_DS = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "model_DS.fit(X_train_ds, y_train_ds)\n",
    "y_pred_ds = model_DS.predict(X_test_ds)\n",
    "mae = mean_absolute_error(y_test_ds, y_pred_ds)\n",
    "r2 = r2_score(y_test_ds, y_pred_ds)\n",
    "print(f\"DS trained ‚Äî MAE: {mae:.3f}h, R2: {r2:.3f}\")\n",
    "# save artifacts for DS\n",
    "joblib.dump(model_DS, 'model_DS.pkl')\n",
    "joblib.dump(scaler_ds, 'scaler_DS.pkl')\n",
    "joblib.dump(le_sender, 'le_sender.pkl')\n",
    "joblib.dump(le_receiver, 'le_receiver.pkl')\n",
    "print(\"Saved DS artifacts: model_DS.pkl, scaler_DS.pkl, le_sender.pkl, le_receiver.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY RELIABILITY (DR) ===\n",
      "DR trained.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60       887\n",
      "           1       0.73      0.67      0.70      1313\n",
      "\n",
      "    accuracy                           0.66      2200\n",
      "   macro avg       0.65      0.65      0.65      2200\n",
      "weighted avg       0.66      0.66      0.66      2200\n",
      "\n",
      "Saved DR artifacts: model_DR_pipeline.pkl, encoder_DR.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY RELIABILITY (DR) ===\")\n",
    "df_dr = pd.read_csv('delivery reliability - DR.csv')\n",
    "df_dr = df_dr.dropna()\n",
    "target = 'Reached.on.Time_Y.N'  # binary 0/1 or 'Y'/'N' as in your data\n",
    "X = df_dr.drop(columns=[target])\n",
    "y = df_dr[target]\n",
    "\n",
    "categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
    "    ('num', StandardScaler(), numeric_cols)\n",
    "])\n",
    "X_train_dr, X_test_dr, y_train_dr, y_test_dr = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "log_pipeline.fit(X_train_dr, y_train_dr)\n",
    "y_pred_dr = log_pipeline.predict(X_test_dr)\n",
    "y_proba_dr = log_pipeline.predict_proba(X_test_dr)[:,1]\n",
    "print(\"DR trained.\")\n",
    "print(classification_report(y_test_dr, y_pred_dr))\n",
    "# save pipeline (easier to use later)\n",
    "joblib.dump(log_pipeline, 'model_DR_pipeline.pkl')\n",
    "# also save preprocessor separately (if needed)\n",
    "joblib.dump(log_pipeline.named_steps['preprocessor'], 'encoder_DR.pkl')\n",
    "print(\"Saved DR artifacts: model_DR_pipeline.pkl, encoder_DR.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b84c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY FLEXIBILITY (DF) ===\n",
      "Flexibility distribution:\n",
      " flexibility_score\n",
      "0     4380\n",
      "1    15583\n",
      "2    23776\n",
      "Name: count, dtype: int64\n",
      "DF trained. Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       876\n",
      "           1       1.00      1.00      1.00      3117\n",
      "           2       1.00      1.00      1.00      4755\n",
      "\n",
      "    accuracy                           1.00      8748\n",
      "   macro avg       1.00      1.00      1.00      8748\n",
      "weighted avg       1.00      1.00      1.00      8748\n",
      "\n",
      "Saved DF artifacts: model_DF.pkl, encoder_DF.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY FLEXIBILITY (DF) ===\")\n",
    "df_df = pd.read_csv('delivery flexibility - amazon_delivery.csv')\n",
    "df_df.columns = [c.strip() for c in df_df.columns]\n",
    "\n",
    "for col in ['Vehicle','Traffic','Weather','Area','Category']:\n",
    "    if col in df_df.columns:\n",
    "        df_df[col] = df_df[col].astype(str).str.strip().str.lower()\n",
    "    else:\n",
    "        df_df[col] = 'unknown'\n",
    "\n",
    "veh_map = {'scooter': 'bike', 'van': 'truck', 'motorcycle': 'motorcycle'}\n",
    "df_df['Vehicle'] = df_df['Vehicle'].map(lambda x: veh_map.get(x, x))\n",
    "area_map = {'metropolitian': 'urban'}\n",
    "df_df['Area'] = df_df['Area'].map(lambda x: area_map.get(x, x))\n",
    "\n",
    "def normalize_weather(w):\n",
    "    w = str(w).lower()\n",
    "    if 'sun' in w: return 'sunny'\n",
    "    if 'cloud' in w: return 'cloudy'\n",
    "    if 'sandstorms' in w: return 'stormy'\n",
    "    return w\n",
    "df_df['Weather'] = df_df['Weather'].map(normalize_weather)\n",
    "\n",
    "def normalize_traffic(t):\n",
    "    t = str(t).lower()\n",
    "    if any(x in t for x in ['jam', 'heavy', 'congest']): return 'jam'\n",
    "    if any(x in t for x in ['low', 'light']): return 'light'\n",
    "    if any(x in t for x in ['medium', 'mod']): return 'medium'\n",
    "    return t\n",
    "df_df['Traffic'] = df_df['Traffic'].map(normalize_traffic)\n",
    "\n",
    "def normalize_cat(c):\n",
    "    s = str(c).lower()\n",
    "    if 'frag' in s or 'glass' in s: return 'fragile'\n",
    "    if 'bulk' in s or 'large' in s: return 'bulky'\n",
    "    return 'regular'\n",
    "df_df['Category'] = df_df['Category'].map(normalize_cat)\n",
    "\n",
    "def flexibility_score(row):\n",
    "    score = 0\n",
    "    if row['Vehicle'] == 'motorcycle': score += 1\n",
    "    if row['Area'] == 'urban': score += 1\n",
    "    if row['Weather'] in ['sunny', 'cloudy']: score += 1\n",
    "    if row['Traffic'] == 'jam': score -= 1\n",
    "    if row['Category'] in ['fragile','bulky']: score -= 1\n",
    "    if score >= 2: return 2\n",
    "    if score == 1: return 1\n",
    "    return 0\n",
    "\n",
    "df_df['flexibility_score'] = df_df.apply(flexibility_score, axis=1)\n",
    "print(\"Flexibility distribution:\\n\", df_df['flexibility_score'].value_counts().sort_index())\n",
    "\n",
    "features_df = ['Vehicle', 'Traffic', 'Area', 'Category', 'Weather']\n",
    "X_df = df_df[features_df]\n",
    "y_df = df_df['flexibility_score']\n",
    "\n",
    "encoder_df = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_df_enc = encoder_df.fit_transform(X_df)\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df_enc, y_df, test_size=0.2, random_state=42, stratify=y_df)\n",
    "clf_df = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "clf_df.fit(X_train_df, y_train_df)\n",
    "y_pred_df = clf_df.predict(X_test_df)\n",
    "print(\"DF trained. Accuracy:\", (y_pred_df == y_test_df).mean())\n",
    "print(classification_report(y_test_df, y_pred_df))\n",
    "\n",
    "# save DF artifacts\n",
    "joblib.dump(clf_df, 'model_DF.pkl')\n",
    "joblib.dump(encoder_df, 'encoder_DF.pkl')\n",
    "print(\"Saved DF artifacts: model_DF.pkl, encoder_DF.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184dcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOAD ARTIFACTS & PREPARE ML¬≤ ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== LOAD ARTIFACTS & PREPARE ML¬≤ ===\")\n",
    "model_DS = joblib.load('model_DS.pkl')\n",
    "scaler_DS = joblib.load('scaler_DS.pkl')\n",
    "le_sender = joblib.load('le_sender.pkl')\n",
    "le_receiver = joblib.load('le_receiver.pkl')\n",
    "\n",
    "model_DR_pipeline = joblib.load('model_DR_pipeline.pkl')  # full pipeline (preprocessor + classifier)\n",
    "encoder_DR = joblib.load('encoder_DR.pkl')               # preprocessor object\n",
    "model_DF = joblib.load('model_DF.pkl')\n",
    "encoder_DF = joblib.load('encoder_DF.pkl')\n",
    "\n",
    "# union of features to randomly synthesize inputs for ML¬≤\n",
    "ds_features = features_ds\n",
    "dr_features = X.columns.tolist()        # original DR features (before encoding)\n",
    "df_features = features_df\n",
    "union_features = list(dict.fromkeys(ds_features + dr_features + df_features + ['Gender', 'Warehouse_block', 'Mode_of_Shipment', 'Product_importance']))\n",
    "def build_input_sample_union(context):\n",
    "    # create sample with union_features, random plausible values\n",
    "    s = {}\n",
    "    for col in union_features:\n",
    "        if col in ds_features:\n",
    "            # shippingDistance sensible range (0.5 - 50 km)\n",
    "            if col == 'shippingDistance':\n",
    "                s[col] = np.round(np.random.uniform(0.5, 50.0), 2)\n",
    "            elif col == 'hour_of_day':\n",
    "                s[col] = np.random.randint(0,24)\n",
    "            elif col == 'day_of_week':\n",
    "                s[col] = np.random.randint(0,7)\n",
    "            elif col == 'is_weekend':\n",
    "                s[col] = int(s.get('day_of_week', 0) in [5,6])\n",
    "            elif col in ['sender_district', 'receiver_district']:\n",
    "                # will put string name then transform later\n",
    "                s[col] = np.random.choice(list(le_sender.classes_))\n",
    "            else:\n",
    "                s[col] = np.random.random()\n",
    "        elif col in df_features:\n",
    "            # df categorical\n",
    "            if col == 'Vehicle':\n",
    "                s[col] = np.random.choice(['bike','truck','motorcycle'])\n",
    "            elif col == 'Traffic':\n",
    "                s[col] = np.random.choice(['jam','medium','light'])\n",
    "            elif col == 'Weather':\n",
    "                s[col] = np.random.choice(['sunny','fog','stormy','cloudy'])\n",
    "            elif col == 'Area':\n",
    "                s[col] = np.random.choice(['urban','suburban','rural'])\n",
    "            elif col == 'Category':\n",
    "                s[col] = np.random.choice(['regular','fragile','bulky'])\n",
    "        else:\n",
    "            # DR features or other generic fields\n",
    "            # if categorical known:\n",
    "            if col == 'Gender':\n",
    "                s[col] = np.random.choice(['M','F'])\n",
    "            elif col == 'Warehouse_block':\n",
    "                s[col] = np.random.choice(['A','B','C','D','F'])\n",
    "            elif col == 'Mode_of_Shipment':\n",
    "                s[col] = np.random.choice(['Ship','Flight','Road'])\n",
    "            elif col == 'Product_importance':\n",
    "                s[col] = np.random.choice(['low','medium','high'])\n",
    "            elif col == 'Customer_care_calls':\n",
    "                s[col] = np.random.randint(0,10)\n",
    "            elif col == 'Prior_purchases':\n",
    "                s[col] = np.random.randint(0,10)\n",
    "            elif col == 'Discount_offered':\n",
    "                s[col] = np.random.randint(0,100)\n",
    "            elif col == 'Customer_rating':\n",
    "                s[col] = np.random.randint(1,6)\n",
    "            elif col == 'Cost_of_the_Product':\n",
    "                s[col] = np.random.randint(50,1000)\n",
    "            elif col == 'Weight_in_gms':\n",
    "                s[col] = np.random.randint(100,5000)\n",
    "            elif col == 'ID':\n",
    "                s[col] = np.random.randint(1000, 9999)\n",
    "            else:\n",
    "                s[col] = np.random.random()\n",
    "    # add context\n",
    "    s['weather'] = context.get('weather', 'sunny')\n",
    "    s['traffic'] = context.get('traffic', 'medium')\n",
    "    s['is_peak_hour'] = int(context.get('is_peak_hour', True))\n",
    "    #DS\n",
    "    s['sender_district'] = context.get('sender_district', 'A')\n",
    "    s['receiver_district'] = context.get('sender_district', 'B')\n",
    "    s['shipping_distance'] = context.get('shippingDistance', 15.0)\n",
    "    s['weight'] = context.get('weight', 1200)\n",
    "    s['serviceType'] = context.get('serviceType', 'standard')\n",
    "    s['shipper'] = context.get('shipper', 'P1')\n",
    "    #DR\n",
    "    s['Agent_Age'] = context.get('Agent_Age', 28)\n",
    "    s['Agent_Rating'] = context.get('Agent_Rating', 4.7)\n",
    "    s['Vehicle'] = context.get('Vehicle', 'Motorcycle')\n",
    "    s['Area'] = context.get('Area', 'Urban')\n",
    "    s['Order_Date'] = context.get('Order_Date', '2023-01-01')\n",
    "    s['Order_Time'] = context.get('Order_Time', '10:00:00')\n",
    "    s['Pickup_Time'] = context.get('Pickup_Time', '10:30:00')\n",
    "    #DF\n",
    "    s['Warehouse_block'] = context.get('Warehouse_block', 'A')\n",
    "    s['Mode_of_Shipment'] = context.get('Mode_of_Shipment', 'Road')\n",
    "    s['Customer_care_calls'] = context.get('Customer_care_calls', 3)\n",
    "    s['Customer_rating'] = context.get('Customer_rating', 4)\n",
    "    s['Cost_of_the_Product'] = context.get('Cost_of_the_Product', 150)\n",
    "    s['Prior_purchases'] = context.get('Prior_purchases', 2)\n",
    "    s['Product_importance'] = context.get('Product_importance', 'medium')\n",
    "    s['Gender'] = context.get('Gender', 'M')\n",
    "    s['Discount_offered'] = context.get('Discount_offered', 10)\n",
    "    s['Weight_in_gms'] = context.get('Weight_in_gms', 1200)\n",
    "    return pd.DataFrame([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5eb12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN ML¬≤ ===\n",
      "ML¬≤ trained. Accuracy: 0.985\n",
      "Saved ML¬≤ model (ml2_model.pkl) and feature list (ml2_features.pkl)\n",
      "\n",
      "=== Pareto Frontier for Context ===\n",
      "{'weather': 'sunny', 'traffic': 'jam', 'is_peak_hour': True}\n",
      "     w1   w2   w3  DS_total  DR_total  DF_total\n",
      "0   0.0  0.0  1.0     9.057     0.801     0.547\n",
      "1   0.0  0.1  0.9     9.988     0.785     0.561\n",
      "2   0.0  0.2  0.8    10.920     0.769     0.576\n",
      "3   0.0  0.3  0.7    11.851     0.752     0.591\n",
      "4   0.0  0.4  0.6    12.783     0.736     0.605\n",
      "..  ...  ...  ...       ...       ...       ...\n",
      "61  0.8  0.1  0.1    13.880     0.763     0.561\n",
      "62  0.8  0.2  0.0    14.811     0.747     0.576\n",
      "63  0.9  0.0  0.1    13.435     0.776     0.547\n",
      "64  0.9  0.1  0.0    14.366     0.760     0.561\n",
      "65  1.0  0.0  0.0    13.921     0.774     0.547\n",
      "\n",
      "[66 rows x 6 columns]\n",
      "\n",
      "=== üèÜ L·ª∞A CH·ªåN T·ªêI ∆ØU THEO T·ª™NG TI√äU CH√ç ===\n",
      "- Nhanh nh·∫•t (DS th·∫•p nh·∫•t): {'w1': np.float64(0.0), 'w2': np.float64(0.0), 'w3': np.float64(1.0), 'DS_total': np.float64(9.05711417608023), 'DR_total': np.float64(0.8012423005820077), 'DF_total': np.float64(0.5466666666666666)}\n",
      "- ƒê√∫ng h·∫°n nh·∫•t (DR cao nh·∫•t): {'w1': np.float64(0.0), 'w2': np.float64(0.0), 'w3': np.float64(1.0), 'DS_total': np.float64(9.05711417608023), 'DR_total': np.float64(0.8012423005820077), 'DF_total': np.float64(0.5466666666666666)}\n",
      "- H√†i l√≤ng nh·∫•t (DF cao nh·∫•t): {'w1': np.float64(0.0), 'w2': np.float64(1.0), 'w3': np.float64(0.0), 'DS_total': np.float64(18.370672125000024), 'DR_total': np.float64(0.6387235827276997), 'DF_total': np.float64(0.6933333333333334)}\n",
      "\n",
      "=== ‚öñÔ∏è TRADE-OFF GI·ªÆA NHANH NH·∫§T vs ƒê√öNG H·∫†N NH·∫§T ===\n",
      "{'DS_total': np.float64(0.0), 'DR_total': np.float64(0.0), 'DF_total': np.float64(0.0)}\n",
      "\n",
      "=== ‚öñÔ∏è TRADE-OFF GI·ªÆA NHANH NH·∫§T vs H√ÄI L√íNG NH·∫§T ===\n",
      "{'DS_total': np.float64(9.314), 'DR_total': np.float64(-0.163), 'DF_total': np.float64(0.147)}\n",
      "\n",
      "=== ‚öñÔ∏è PH∆Ø∆†NG √ÅN C√ÇN B·∫∞NG NH·∫§T (Theo Kho·∫£ng C√°ch T·ªõi ƒêi·ªÉm L√Ω T∆∞·ªüng) ===\n",
      "Ph∆∞∆°ng √°n c√¢n b·∫±ng nh·∫•t:\n",
      "w1           0.000000\n",
      "w2           0.300000\n",
      "w3           0.700000\n",
      "DS_total    11.851182\n",
      "DR_total     0.752487\n",
      "DF_total     0.590667\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def predict_models_union(context, policy_id, model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF):\n",
    "    sample = build_input_sample_union(context)\n",
    "    sample_ds = sample[ds_features].copy()\n",
    "    try:\n",
    "        sample_ds['sender_district'] = le_sender.transform(sample_ds['sender_district'].astype(str))\n",
    "    except Exception:\n",
    "        sample_ds['sender_district'] = 0\n",
    "    try:\n",
    "        sample_ds['receiver_district'] = le_receiver.transform(sample_ds['receiver_district'].astype(str))\n",
    "    except Exception:\n",
    "        sample_ds['receiver_district'] = 0\n",
    "    sample_ds_scaled = scaler_DS.transform(sample_ds)\n",
    "    ds_pred = float(model_DS.predict(sample_ds_scaled)[0])  # hours\n",
    "     # --- DR input ---\n",
    "    sample_dr = sample[dr_features].copy()\n",
    "    # use full pipeline (preprocessor + classifier) to get probability of on-time\n",
    "    dr_proba = float(model_DR_pipeline.predict_proba(sample_dr)[:,1][0])\n",
    "    # --- DF input ---\n",
    "    sample_df = sample[df_features].copy()\n",
    "    # encoder_DF expects original df_features\n",
    "    sample_df_enc = encoder_DF.transform(sample_df)\n",
    "    # get predicted class (0,1,2) or we can use expected value via predict_proba\n",
    "    probs_df = model_DF.predict_proba(sample_df_enc)[0]  # probabilities for classes sorted by classifier classes_\n",
    "    # convert to expected numeric score: classes might be [0,1,2]\n",
    "    classes = model_DF.classes_\n",
    "    df_expected = float(np.sum(classes * probs_df))\n",
    "    return {\n",
    "        'policy': policy_id,\n",
    "        'DS_total': ds_pred,\n",
    "        'DR_total': dr_proba,\n",
    "        'DF_total': df_expected\n",
    "    }\n",
    "def generate_allocations(step=0.2):\n",
    "    vals = np.arange(0, 1 + 1e-9, step)\n",
    "    allocs = []\n",
    "    for w1, w2, w3 in product(vals, repeat=3):\n",
    "        if abs(w1 + w2 + w3 - 1) < 1e-6:\n",
    "            allocs.append((round(w1,2), round(w2,2), round(w3,2)))\n",
    "    return allocs\n",
    "def compute_models_for_allocs(context, allocations, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                              model_DR_pipeline, model_DF, encoder_DF):\n",
    "    policies = ['P1','P2','P3']\n",
    "    per_policy = {}\n",
    "    for pid in policies:\n",
    "        per_policy[pid] = predict_models_union(context, pid, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                                               model_DR_pipeline, model_DF, encoder_DF)\n",
    "    rows = []\n",
    "    for (w1,w2,w3) in allocations:\n",
    "        DS_total = w1*per_policy['P1']['DS_total'] + w2*per_policy['P2']['DS_total'] + w3*per_policy['P3']['DS_total']\n",
    "        DR_total = w1*per_policy['P1']['DR_total'] + w2*per_policy['P2']['DR_total'] + w3*per_policy['P3']['DR_total']\n",
    "        DF_total = w1*per_policy['P1']['DF_total'] + w2*per_policy['P2']['DF_total'] + w3*per_policy['P3']['DF_total']\n",
    "        rows.append({'w1':w1,'w2':w2,'w3':w3,'DS_total':DS_total,'DR_total':DR_total,'DF_total':DF_total})\n",
    "    return pd.DataFrame(rows)\n",
    "def pareto_front(df, cols):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "    for i, row in df.iterrows():\n",
    "        dominated = ((df[cols] >= row[cols]).all(axis=1)) & ((df[cols] > row[cols]).any(axis=1))\n",
    "        if dominated.any():\n",
    "            mask[i] = False\n",
    "    return mask\n",
    "def synthesize_ml2_data(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF,\n",
    "                        n_samples=500, step=0.1):\n",
    "    rng = np.random.RandomState(42)\n",
    "    records = []\n",
    "    allocs = generate_allocations(step)\n",
    "    for _ in range(n_samples):\n",
    "        context = {\n",
    "            'weather': rng.choice(['sunny','fog','stormy','cloudy']),\n",
    "            'traffic': rng.choice(['jam','medium','light']),\n",
    "            'is_peak_hour': bool(rng.choice([0,1], p=[0.7,0.3])),\n",
    "            #DS\n",
    "            'sender_district': rng.choice(['Qu·∫≠n 1', 'Qu·∫≠n 5', 'B√¨nh Th·∫°nh', 'T√¢n B√¨nh']),\n",
    "            'receiver_district': rng.choice(['Qu·∫≠n 1', 'Qu·∫≠n 5', 'B√¨nh Th·∫°nh', 'T√¢n B√¨nh', 'Qu·∫≠n 10']),\n",
    "            'shipping_distance': rng.uniform(5,50),\n",
    "            'weight': rng.uniform(500, 5000),\n",
    "            'serviceType': rng.choice(['standard', 'express']),\n",
    "            #DR\n",
    "            'Agent_Age': rng.randint(20,60),\n",
    "            'Agent_Rating': rng.uniform(3.0, 5.0),\n",
    "            'Vehicle': rng.choice(['Bike', 'Car', 'Truck']),\n",
    "            'Area': rng.choice(['Urban', 'Rural']),\n",
    "            'Order_Date': '2024-01-01',\n",
    "            'Order_Time': rng.choice(['08:00:00', '12:00:00', '18:00:00']),\n",
    "            'Pickup_Time': rng.choice(['08:30:00', '12:30:00', '18:30:00']),\n",
    "            #DF\n",
    "            'Warehouse_block': rng.choice(['A', 'B', 'C', 'D']),\n",
    "            'Mode_of_Shipment': rng.choice(['Ship', 'Flight', 'Road']),\n",
    "            'Customer_care_calls': rng.randint(1, 7),\n",
    "            'Customer_rating': rng.randint(1, 5),\n",
    "    'Cost_of_the_Product': rng.randint(100, 500),\n",
    "    'Prior_purchases': rng.randint(0, 5),\n",
    "    'Product_importance': rng.choice(['low', 'medium', 'high']),\n",
    "    'Gender': rng.choice(['M', 'F']),\n",
    "    'Discount_offered': rng.randint(0, 30),\n",
    "    'Weight_in_gms': rng.uniform(300, 4000)\n",
    "        }\n",
    "        df_alloc = compute_models_for_allocs(context, allocs, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                                             model_DR_pipeline, model_DF, encoder_DF)\n",
    "        mask = pareto_front(df_alloc, ['DS_total','DR_total','DF_total'])\n",
    "        pareto = df_alloc[mask].reset_index(drop=True)\n",
    "        # choose based on context heuristics\n",
    "        if context['weather'] in ('stormy','cloudy') or context['traffic']=='jam':\n",
    "            chosen = pareto.loc[pareto['DR_total'].idxmax()]\n",
    "        elif context['is_peak_hour']:\n",
    "            chosen = pareto.loc[pareto['DS_total'].idxmax()]\n",
    "        else:\n",
    "            chosen = pareto.loc[pareto['DF_total'].idxmax()]\n",
    "        rec = {\n",
    "            **context,\n",
    "            'w1': chosen['w1'], 'w2': chosen['w2'], 'w3': chosen['w3'],\n",
    "            'DS_total': chosen['DS_total'], 'DR_total': chosen['DR_total'], 'DF_total': chosen['DF_total']\n",
    "        }\n",
    "        records.append(rec)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def train_ml2(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF):\n",
    "    df_train = synthesize_ml2_data(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF,\n",
    "                                   n_samples=1000, step=0.1)\n",
    "    X = df_train[['weather','traffic','is_peak_hour','DS_total','DR_total','DF_total','w1','w2','w3']].copy()\n",
    "    y = df_train[['w1','w2','w3']].apply(lambda r: f\"{r.w1}_{r.w2}_{r.w3}\", axis=1)\n",
    "    X = pd.get_dummies(X, columns=['weather','traffic'], drop_first=True)\n",
    "    X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf_m2 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf_m2.fit(X_train_m2, y_train_m2)\n",
    "    print(\"ML¬≤ trained. Accuracy:\", clf_m2.score(X_test_m2, y_test_m2))\n",
    "    return clf_m2, X.columns.tolist()\n",
    "print(\"=== TRAIN ML¬≤ ===\")\n",
    "ml2_model, ml2_features = train_ml2(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF)\n",
    "joblib.dump(ml2_model, 'ml2_model.pkl')\n",
    "joblib.dump(ml2_features, 'ml2_features.pkl')\n",
    "print(\"Saved ML¬≤ model (ml2_model.pkl) and feature list (ml2_features.pkl)\\n\")\n",
    "\n",
    "ctx = {'weather':'sunny','traffic':'jam','is_peak_hour':True}\n",
    "allocs = generate_allocations(0.1)\n",
    "df_alloc_test = compute_models_for_allocs(ctx, allocs, model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF)\n",
    "mask = pareto_front(df_alloc_test, ['DS_total','DR_total','DF_total'])\n",
    "pareto_df = df_alloc_test[mask].reset_index(drop = True)\n",
    "\n",
    "print(\"=== Pareto Frontier for Context ===\")\n",
    "print(ctx)\n",
    "print(pareto_df.round(3))\n",
    "\n",
    "best_speed = pareto_df.loc[pareto_df['DS_total'].idxmin()]\n",
    "best_reliab = pareto_df.loc[pareto_df['DR_total'].idxmax()]\n",
    "best_feedback = pareto_df.loc[pareto_df['DF_total'].idxmax()]\n",
    "\n",
    "print(\"\\n=== üèÜ L·ª∞A CH·ªåN T·ªêI ∆ØU THEO T·ª™NG TI√äU CH√ç ===\")\n",
    "print(f\"- Nhanh nh·∫•t (DS th·∫•p nh·∫•t): {dict(best_speed)}\")\n",
    "print(f\"- ƒê√∫ng h·∫°n nh·∫•t (DR cao nh·∫•t): {dict(best_reliab)}\")\n",
    "print(f\"- H√†i l√≤ng nh·∫•t (DF cao nh·∫•t): {dict(best_feedback)}\")\n",
    "\n",
    "\n",
    "# === T√çNH TRADE-OFF GI·ªÆA C√ÅC L·ª∞A CH·ªåN ===\n",
    "def diff(a, b):\n",
    "    return {k: round(b[k] - a[k], 3) for k in ['DS_total', 'DR_total', 'DF_total']}\n",
    "\n",
    "print(\"\\n=== ‚öñÔ∏è TRADE-OFF GI·ªÆA NHANH NH·∫§T vs ƒê√öNG H·∫†N NH·∫§T ===\")\n",
    "print(diff(best_speed, best_reliab))\n",
    "print(\"\\n=== ‚öñÔ∏è TRADE-OFF GI·ªÆA NHANH NH·∫§T vs H√ÄI L√íNG NH·∫§T ===\")\n",
    "print(diff(best_speed, best_feedback))\n",
    "\n",
    "\n",
    "print(\"\\n=== ‚öñÔ∏è PH∆Ø∆†NG √ÅN C√ÇN B·∫∞NG NH·∫§T (Theo Kho·∫£ng C√°ch T·ªõi ƒêi·ªÉm L√Ω T∆∞·ªüng) ===\")\n",
    "\n",
    "# Chu·∫©n h√≥a c√°c ti√™u ch√≠ (ƒë∆∞a v·ªÅ c√πng thang ƒëo)\n",
    "df_norm = pareto_df.copy()\n",
    "df_norm['DS_norm'] = (df_norm['DS_total'] - df_norm['DS_total'].min()) / (df_norm['DS_total'].max() - df_norm['DS_total'].min())\n",
    "df_norm['DR_norm'] = (df_norm['DR_total'] - df_norm['DR_total'].min()) / (df_norm['DR_total'].max() - df_norm['DR_total'].min())\n",
    "df_norm['DF_norm'] = (df_norm['DF_total'] - df_norm['DF_total'].min()) / (df_norm['DF_total'].max() - df_norm['DF_total'].min())\n",
    "\n",
    "# X√°c ƒë·ªãnh h∆∞·ªõng t·ªëi ∆∞u:\n",
    "# - DS: c√†ng th·∫•p c√†ng t·ªët  ‚Üí gi√° tr·ªã l√Ω t∆∞·ªüng l√† 0\n",
    "# - DR, DF: c√†ng cao c√†ng t·ªët ‚Üí gi√° tr·ªã l√Ω t∆∞·ªüng l√† 1\n",
    "ideal_point = np.array([0, 1, 1])\n",
    "# T√≠nh kho·∫£ng c√°ch Euclidean t·ªõi ƒëi·ªÉm l√Ω t∆∞·ªüng\n",
    "df_norm['distance_to_ideal'] = np.sqrt(\n",
    "    (df_norm['DS_norm'] - ideal_point[0])**2 +\n",
    "    (df_norm['DR_norm'] - ideal_point[1])**2 +\n",
    "    (df_norm['DF_norm'] - ideal_point[2])**2\n",
    ")\n",
    "\n",
    "# Ch·ªçn ph∆∞∆°ng √°n c√≥ kho·∫£ng c√°ch nh·ªè nh·∫•t (g·∫ßn ƒëi·ªÉm l√Ω t∆∞·ªüng nh·∫•t)\n",
    "best_compromise_idx = df_norm['distance_to_ideal'].idxmin()\n",
    "best_compromise = pareto_df.loc[best_compromise_idx]\n",
    "print(\"Ph∆∞∆°ng √°n c√¢n b·∫±ng nh·∫•t:\")\n",
    "print(best_compromise)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
