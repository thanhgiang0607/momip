{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac7e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2080ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY SPEED (DS) ===\n",
      "DS trained — MAE: 2.459h, R2: 0.520\n",
      "Saved DS artifacts: model_DS.pkl, scaler_DS.pkl, le_sender.pkl, le_receiver.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY SPEED (DS) ===\")\n",
    "df_ds = pd.read_csv('delivery speed - uds-orders-aug2024.csv')\n",
    "df_ds = df_ds.dropna(subset = ['deliveredAt', 'createdAt'])\n",
    "df_ds['deliveredAt'] = pd.to_datetime(df_ds['deliveredAt'], errors = 'coerce')\n",
    "df_ds['createdAt'] = pd.to_datetime(df_ds['createdAt'], errors = 'coerce')\n",
    "df_ds = df_ds.dropna(subset=['createdAt', 'deliveredAt'])\n",
    "df_ds['delivery_duration_hours'] = (df_ds['deliveredAt'] - df_ds['createdAt']).dt.total_seconds() / 3600\n",
    "df_ds = df_ds[df_ds['delivery_duration_hours'].between(0.1, 72)]\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
    "    return 2 * R * atan2(sqrt(a), sqrt(1 - a))\n",
    "def extract_district(address):\n",
    "    if pd.isna(address):\n",
    "        return 'unknown'\n",
    "    try: \n",
    "        match = re.search(r\"(Quận|Q\\.?|Huyện|H\\.?)\\s*([\\w\\s\\dĐđÂâĂăÊêÔôƠơƯư]+)\", str(address))\n",
    "        if match:\n",
    "            return match.group(0).strip()\n",
    "    except re.error:\n",
    "        return \"Unknown\"\n",
    "    return \"Unknown\"\n",
    "if 'senderAddress' in df_ds.columns and 'receiverAddress' in df_ds.columns:\n",
    "    df_ds[\"sender_district\"] = df_ds['senderAddress'].apply(extract_district)\n",
    "    df_ds['receiver_district'] = df_ds['receiverAddress'].apply(extract_district)\n",
    "else:\n",
    "    df_ds['sender_district'] = \"Unknown\"\n",
    "    df_ds['receiver_district'] = \"Unknown\"\n",
    "df_ds['hour_of_day'] = df_ds['createdAt'].dt.hour\n",
    "df_ds['day_of_week'] = df_ds['createdAt'].dt.dayofweek\n",
    "df_ds['is_weekend'] = df_ds['day_of_week'].isin([5,6]).astype(int)\n",
    "# encoders for district\n",
    "le_sender = LabelEncoder()\n",
    "le_receiver = LabelEncoder()\n",
    "df_ds['sender_district'] = le_sender.fit_transform(df_ds['sender_district'].astype(str))\n",
    "df_ds['receiver_district'] = le_receiver.fit_transform(df_ds['receiver_district'].astype(str))  # FIXED: use le_receiver here\n",
    "features_ds = ['shippingDistance', 'hour_of_day', 'day_of_week', 'sender_district', 'receiver_district', 'is_weekend']\n",
    "X_ds = df_ds[features_ds].copy()\n",
    "y_ds = df_ds['delivery_duration_hours']\n",
    "scaler_ds = StandardScaler()\n",
    "X_ds_scaled = scaler_ds.fit_transform(X_ds)\n",
    "\n",
    "X_train_ds, X_test_ds, y_train_ds, y_test_ds = train_test_split(X_ds_scaled, y_ds, test_size=0.2, random_state=42)\n",
    "model_DS = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "model_DS.fit(X_train_ds, y_train_ds)\n",
    "y_pred_ds = model_DS.predict(X_test_ds)\n",
    "mae = mean_absolute_error(y_test_ds, y_pred_ds)\n",
    "r2 = r2_score(y_test_ds, y_pred_ds)\n",
    "print(f\"DS trained — MAE: {mae:.3f}h, R2: {r2:.3f}\")\n",
    "# save artifacts for DS\n",
    "joblib.dump(model_DS, 'model_DS.pkl')\n",
    "joblib.dump(scaler_ds, 'scaler_DS.pkl')\n",
    "joblib.dump(le_sender, 'le_sender.pkl')\n",
    "joblib.dump(le_receiver, 'le_receiver.pkl')\n",
    "print(\"Saved DS artifacts: model_DS.pkl, scaler_DS.pkl, le_sender.pkl, le_receiver.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY RELIABILITY (DR) ===\n",
      "DR trained.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60       887\n",
      "           1       0.73      0.67      0.70      1313\n",
      "\n",
      "    accuracy                           0.66      2200\n",
      "   macro avg       0.65      0.65      0.65      2200\n",
      "weighted avg       0.66      0.66      0.66      2200\n",
      "\n",
      "Saved DR artifacts: model_DR_pipeline.pkl, encoder_DR.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY RELIABILITY (DR) ===\")\n",
    "df_dr = pd.read_csv('delivery reliability - DR.csv')\n",
    "df_dr = df_dr.dropna()\n",
    "target = 'Reached.on.Time_Y.N'  # binary 0/1 or 'Y'/'N' as in your data\n",
    "X = df_dr.drop(columns=[target])\n",
    "y = df_dr[target]\n",
    "\n",
    "categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
    "    ('num', StandardScaler(), numeric_cols)\n",
    "])\n",
    "X_train_dr, X_test_dr, y_train_dr, y_test_dr = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "log_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "log_pipeline.fit(X_train_dr, y_train_dr)\n",
    "y_pred_dr = log_pipeline.predict(X_test_dr)\n",
    "y_proba_dr = log_pipeline.predict_proba(X_test_dr)[:,1]\n",
    "print(\"DR trained.\")\n",
    "print(classification_report(y_test_dr, y_pred_dr))\n",
    "# save pipeline (easier to use later)\n",
    "joblib.dump(log_pipeline, 'model_DR_pipeline.pkl')\n",
    "# also save preprocessor separately (if needed)\n",
    "joblib.dump(log_pipeline.named_steps['preprocessor'], 'encoder_DR.pkl')\n",
    "print(\"Saved DR artifacts: model_DR_pipeline.pkl, encoder_DR.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b84c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DELIVERY FLEXIBILITY (DF) ===\n",
      "Flexibility distribution:\n",
      " flexibility_score\n",
      "0     4380\n",
      "1    15583\n",
      "2    23776\n",
      "Name: count, dtype: int64\n",
      "DF trained. Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       876\n",
      "           1       1.00      1.00      1.00      3117\n",
      "           2       1.00      1.00      1.00      4755\n",
      "\n",
      "    accuracy                           1.00      8748\n",
      "   macro avg       1.00      1.00      1.00      8748\n",
      "weighted avg       1.00      1.00      1.00      8748\n",
      "\n",
      "Saved DF artifacts: model_DF.pkl, encoder_DF.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN DELIVERY FLEXIBILITY (DF) ===\")\n",
    "df_df = pd.read_csv('delivery flexibility - amazon_delivery.csv')\n",
    "df_df.columns = [c.strip() for c in df_df.columns]\n",
    "\n",
    "for col in ['Vehicle','Traffic','Weather','Area','Category']:\n",
    "    if col in df_df.columns:\n",
    "        df_df[col] = df_df[col].astype(str).str.strip().str.lower()\n",
    "    else:\n",
    "        df_df[col] = 'unknown'\n",
    "\n",
    "veh_map = {'scooter': 'bike', 'van': 'truck', 'motorcycle': 'motorcycle'}\n",
    "df_df['Vehicle'] = df_df['Vehicle'].map(lambda x: veh_map.get(x, x))\n",
    "area_map = {'metropolitian': 'urban'}\n",
    "df_df['Area'] = df_df['Area'].map(lambda x: area_map.get(x, x))\n",
    "\n",
    "def normalize_weather(w):\n",
    "    w = str(w).lower()\n",
    "    if 'sun' in w: return 'sunny'\n",
    "    if 'cloud' in w: return 'cloudy'\n",
    "    if 'sandstorms' in w: return 'stormy'\n",
    "    return w\n",
    "df_df['Weather'] = df_df['Weather'].map(normalize_weather)\n",
    "\n",
    "def normalize_traffic(t):\n",
    "    t = str(t).lower()\n",
    "    if any(x in t for x in ['jam', 'heavy', 'congest']): return 'jam'\n",
    "    if any(x in t for x in ['low', 'light']): return 'light'\n",
    "    if any(x in t for x in ['medium', 'mod']): return 'medium'\n",
    "    return t\n",
    "df_df['Traffic'] = df_df['Traffic'].map(normalize_traffic)\n",
    "\n",
    "def normalize_cat(c):\n",
    "    s = str(c).lower()\n",
    "    if 'frag' in s or 'glass' in s: return 'fragile'\n",
    "    if 'bulk' in s or 'large' in s: return 'bulky'\n",
    "    return 'regular'\n",
    "df_df['Category'] = df_df['Category'].map(normalize_cat)\n",
    "\n",
    "def flexibility_score(row):\n",
    "    score = 0\n",
    "    if row['Vehicle'] == 'motorcycle': score += 1\n",
    "    if row['Area'] == 'urban': score += 1\n",
    "    if row['Weather'] in ['sunny', 'cloudy']: score += 1\n",
    "    if row['Traffic'] == 'jam': score -= 1\n",
    "    if row['Category'] in ['fragile','bulky']: score -= 1\n",
    "    if score >= 2: return 2\n",
    "    if score == 1: return 1\n",
    "    return 0\n",
    "\n",
    "df_df['flexibility_score'] = df_df.apply(flexibility_score, axis=1)\n",
    "print(\"Flexibility distribution:\\n\", df_df['flexibility_score'].value_counts().sort_index())\n",
    "\n",
    "features_df = ['Vehicle', 'Traffic', 'Area', 'Category', 'Weather']\n",
    "X_df = df_df[features_df]\n",
    "y_df = df_df['flexibility_score']\n",
    "\n",
    "encoder_df = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_df_enc = encoder_df.fit_transform(X_df)\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df_enc, y_df, test_size=0.2, random_state=42, stratify=y_df)\n",
    "clf_df = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "clf_df.fit(X_train_df, y_train_df)\n",
    "y_pred_df = clf_df.predict(X_test_df)\n",
    "print(\"DF trained. Accuracy:\", (y_pred_df == y_test_df).mean())\n",
    "print(classification_report(y_test_df, y_pred_df))\n",
    "\n",
    "# save DF artifacts\n",
    "joblib.dump(clf_df, 'model_DF.pkl')\n",
    "joblib.dump(encoder_df, 'encoder_DF.pkl')\n",
    "print(\"Saved DF artifacts: model_DF.pkl, encoder_DF.pkl\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184dcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOAD ARTIFACTS & PREPARE ML² ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== LOAD ARTIFACTS & PREPARE ML² ===\")\n",
    "model_DS = joblib.load('model_DS.pkl')\n",
    "scaler_DS = joblib.load('scaler_DS.pkl')\n",
    "le_sender = joblib.load('le_sender.pkl')\n",
    "le_receiver = joblib.load('le_receiver.pkl')\n",
    "\n",
    "model_DR_pipeline = joblib.load('model_DR_pipeline.pkl')  # full pipeline (preprocessor + classifier)\n",
    "encoder_DR = joblib.load('encoder_DR.pkl')               # preprocessor object\n",
    "model_DF = joblib.load('model_DF.pkl')\n",
    "encoder_DF = joblib.load('encoder_DF.pkl')\n",
    "\n",
    "# union of features to randomly synthesize inputs for ML²\n",
    "ds_features = features_ds\n",
    "dr_features = X.columns.tolist()        # original DR features (before encoding)\n",
    "df_features = features_df\n",
    "union_features = list(dict.fromkeys(ds_features + dr_features + df_features + ['Gender', 'Warehouse_block', 'Mode_of_Shipment', 'Product_importance']))\n",
    "def build_input_sample_union(context):\n",
    "    # create sample with union_features, random plausible values\n",
    "    s = {}\n",
    "    for col in union_features:\n",
    "        if col in ds_features:\n",
    "            # shippingDistance sensible range (0.5 - 50 km)\n",
    "            if col == 'shippingDistance':\n",
    "                s[col] = np.round(np.random.uniform(0.5, 50.0), 2)\n",
    "            elif col == 'hour_of_day':\n",
    "                s[col] = np.random.randint(0,24)\n",
    "            elif col == 'day_of_week':\n",
    "                s[col] = np.random.randint(0,7)\n",
    "            elif col == 'is_weekend':\n",
    "                s[col] = int(s.get('day_of_week', 0) in [5,6])\n",
    "            elif col in ['sender_district', 'receiver_district']:\n",
    "                # will put string name then transform later\n",
    "                s[col] = np.random.choice(list(le_sender.classes_))\n",
    "            else:\n",
    "                s[col] = np.random.random()\n",
    "        elif col in df_features:\n",
    "            # df categorical\n",
    "            if col == 'Vehicle':\n",
    "                s[col] = np.random.choice(['bike','truck','motorcycle'])\n",
    "            elif col == 'Traffic':\n",
    "                s[col] = np.random.choice(['jam','medium','light'])\n",
    "            elif col == 'Weather':\n",
    "                s[col] = np.random.choice(['sunny','fog','stormy','cloudy'])\n",
    "            elif col == 'Area':\n",
    "                s[col] = np.random.choice(['urban','suburban','rural'])\n",
    "            elif col == 'Category':\n",
    "                s[col] = np.random.choice(['regular','fragile','bulky'])\n",
    "        else:\n",
    "            # DR features or other generic fields\n",
    "            # if categorical known:\n",
    "            if col == 'Gender':\n",
    "                s[col] = np.random.choice(['M','F'])\n",
    "            elif col == 'Warehouse_block':\n",
    "                s[col] = np.random.choice(['A','B','C','D','F'])\n",
    "            elif col == 'Mode_of_Shipment':\n",
    "                s[col] = np.random.choice(['Ship','Flight','Road'])\n",
    "            elif col == 'Product_importance':\n",
    "                s[col] = np.random.choice(['low','medium','high'])\n",
    "            elif col == 'Customer_care_calls':\n",
    "                s[col] = np.random.randint(0,10)\n",
    "            elif col == 'Prior_purchases':\n",
    "                s[col] = np.random.randint(0,10)\n",
    "            elif col == 'Discount_offered':\n",
    "                s[col] = np.random.randint(0,100)\n",
    "            elif col == 'Customer_rating':\n",
    "                s[col] = np.random.randint(1,6)\n",
    "            elif col == 'Cost_of_the_Product':\n",
    "                s[col] = np.random.randint(50,1000)\n",
    "            elif col == 'Weight_in_gms':\n",
    "                s[col] = np.random.randint(100,5000)\n",
    "            elif col == 'ID':\n",
    "                s[col] = np.random.randint(1000, 9999)\n",
    "            else:\n",
    "                s[col] = np.random.random()\n",
    "    # add context\n",
    "    s['weather'] = context.get('weather', 'sunny')\n",
    "    s['traffic'] = context.get('traffic', 'medium')\n",
    "    s['is_peak_hour'] = int(context.get('is_peak_hour', True))\n",
    "    #DS\n",
    "    s['sender_district'] = context.get('sender_district', 'A')\n",
    "    s['receiver_district'] = context.get('sender_district', 'B')\n",
    "    s['shipping_distance'] = context.get('shippingDistance', 15.0)\n",
    "    s['weight'] = context.get('weight', 1200)\n",
    "    s['serviceType'] = context.get('serviceType', 'standard')\n",
    "    s['shipper'] = context.get('shipper', 'P1')\n",
    "    #DR\n",
    "    s['Agent_Age'] = context.get('Agent_Age', 28)\n",
    "    s['Agent_Rating'] = context.get('Agent_Rating', 4.7)\n",
    "    s['Vehicle'] = context.get('Vehicle', 'Motorcycle')\n",
    "    s['Area'] = context.get('Area', 'Urban')\n",
    "    s['Order_Date'] = context.get('Order_Date', '2023-01-01')\n",
    "    s['Order_Time'] = context.get('Order_Time', '10:00:00')\n",
    "    s['Pickup_Time'] = context.get('Pickup_Time', '10:30:00')\n",
    "    #DF\n",
    "    s['Warehouse_block'] = context.get('Warehouse_block', 'A')\n",
    "    s['Mode_of_Shipment'] = context.get('Mode_of_Shipment', 'Road')\n",
    "    s['Customer_care_calls'] = context.get('Customer_care_calls', 3)\n",
    "    s['Customer_rating'] = context.get('Customer_rating', 4)\n",
    "    s['Cost_of_the_Product'] = context.get('Cost_of_the_Product', 150)\n",
    "    s['Prior_purchases'] = context.get('Prior_purchases', 2)\n",
    "    s['Product_importance'] = context.get('Product_importance', 'medium')\n",
    "    s['Gender'] = context.get('Gender', 'M')\n",
    "    s['Discount_offered'] = context.get('Discount_offered', 10)\n",
    "    s['Weight_in_gms'] = context.get('Weight_in_gms', 1200)\n",
    "    return pd.DataFrame([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5eb12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN ML² ===\n",
      "ML² trained. Accuracy: 0.985\n",
      "Saved ML² model (ml2_model.pkl) and feature list (ml2_features.pkl)\n",
      "\n",
      "=== Pareto Frontier for Context ===\n",
      "{'weather': 'sunny', 'traffic': 'jam', 'is_peak_hour': True}\n",
      "     w1   w2   w3  DS_total  DR_total  DF_total\n",
      "0   0.0  0.0  1.0     9.057     0.801     0.547\n",
      "1   0.0  0.1  0.9     9.988     0.785     0.561\n",
      "2   0.0  0.2  0.8    10.920     0.769     0.576\n",
      "3   0.0  0.3  0.7    11.851     0.752     0.591\n",
      "4   0.0  0.4  0.6    12.783     0.736     0.605\n",
      "..  ...  ...  ...       ...       ...       ...\n",
      "61  0.8  0.1  0.1    13.880     0.763     0.561\n",
      "62  0.8  0.2  0.0    14.811     0.747     0.576\n",
      "63  0.9  0.0  0.1    13.435     0.776     0.547\n",
      "64  0.9  0.1  0.0    14.366     0.760     0.561\n",
      "65  1.0  0.0  0.0    13.921     0.774     0.547\n",
      "\n",
      "[66 rows x 6 columns]\n",
      "\n",
      "=== 🏆 LỰA CHỌN TỐI ƯU THEO TỪNG TIÊU CHÍ ===\n",
      "- Nhanh nhất (DS thấp nhất): {'w1': np.float64(0.0), 'w2': np.float64(0.0), 'w3': np.float64(1.0), 'DS_total': np.float64(9.05711417608023), 'DR_total': np.float64(0.8012423005820077), 'DF_total': np.float64(0.5466666666666666)}\n",
      "- Đúng hạn nhất (DR cao nhất): {'w1': np.float64(0.0), 'w2': np.float64(0.0), 'w3': np.float64(1.0), 'DS_total': np.float64(9.05711417608023), 'DR_total': np.float64(0.8012423005820077), 'DF_total': np.float64(0.5466666666666666)}\n",
      "- Hài lòng nhất (DF cao nhất): {'w1': np.float64(0.0), 'w2': np.float64(1.0), 'w3': np.float64(0.0), 'DS_total': np.float64(18.370672125000024), 'DR_total': np.float64(0.6387235827276997), 'DF_total': np.float64(0.6933333333333334)}\n",
      "\n",
      "=== ⚖️ TRADE-OFF GIỮA NHANH NHẤT vs ĐÚNG HẠN NHẤT ===\n",
      "{'DS_total': np.float64(0.0), 'DR_total': np.float64(0.0), 'DF_total': np.float64(0.0)}\n",
      "\n",
      "=== ⚖️ TRADE-OFF GIỮA NHANH NHẤT vs HÀI LÒNG NHẤT ===\n",
      "{'DS_total': np.float64(9.314), 'DR_total': np.float64(-0.163), 'DF_total': np.float64(0.147)}\n",
      "\n",
      "=== ⚖️ PHƯƠNG ÁN CÂN BẰNG NHẤT (Theo Khoảng Cách Tới Điểm Lý Tưởng) ===\n",
      "Phương án cân bằng nhất:\n",
      "w1           0.000000\n",
      "w2           0.300000\n",
      "w3           0.700000\n",
      "DS_total    11.851182\n",
      "DR_total     0.752487\n",
      "DF_total     0.590667\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def predict_models_union(context, policy_id, model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF):\n",
    "    sample = build_input_sample_union(context)\n",
    "    sample_ds = sample[ds_features].copy()\n",
    "    try:\n",
    "        sample_ds['sender_district'] = le_sender.transform(sample_ds['sender_district'].astype(str))\n",
    "    except Exception:\n",
    "        sample_ds['sender_district'] = 0\n",
    "    try:\n",
    "        sample_ds['receiver_district'] = le_receiver.transform(sample_ds['receiver_district'].astype(str))\n",
    "    except Exception:\n",
    "        sample_ds['receiver_district'] = 0\n",
    "    sample_ds_scaled = scaler_DS.transform(sample_ds)\n",
    "    ds_pred = float(model_DS.predict(sample_ds_scaled)[0])  # hours\n",
    "     # --- DR input ---\n",
    "    sample_dr = sample[dr_features].copy()\n",
    "    # use full pipeline (preprocessor + classifier) to get probability of on-time\n",
    "    dr_proba = float(model_DR_pipeline.predict_proba(sample_dr)[:,1][0])\n",
    "    # --- DF input ---\n",
    "    sample_df = sample[df_features].copy()\n",
    "    # encoder_DF expects original df_features\n",
    "    sample_df_enc = encoder_DF.transform(sample_df)\n",
    "    # get predicted class (0,1,2) or we can use expected value via predict_proba\n",
    "    probs_df = model_DF.predict_proba(sample_df_enc)[0]  # probabilities for classes sorted by classifier classes_\n",
    "    # convert to expected numeric score: classes might be [0,1,2]\n",
    "    classes = model_DF.classes_\n",
    "    df_expected = float(np.sum(classes * probs_df))\n",
    "    return {\n",
    "        'policy': policy_id,\n",
    "        'DS_total': ds_pred,\n",
    "        'DR_total': dr_proba,\n",
    "        'DF_total': df_expected\n",
    "    }\n",
    "def generate_allocations(step=0.2):\n",
    "    vals = np.arange(0, 1 + 1e-9, step)\n",
    "    allocs = []\n",
    "    for w1, w2, w3 in product(vals, repeat=3):\n",
    "        if abs(w1 + w2 + w3 - 1) < 1e-6:\n",
    "            allocs.append((round(w1,2), round(w2,2), round(w3,2)))\n",
    "    return allocs\n",
    "def compute_models_for_allocs(context, allocations, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                              model_DR_pipeline, model_DF, encoder_DF):\n",
    "    policies = ['P1','P2','P3']\n",
    "    per_policy = {}\n",
    "    for pid in policies:\n",
    "        per_policy[pid] = predict_models_union(context, pid, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                                               model_DR_pipeline, model_DF, encoder_DF)\n",
    "    rows = []\n",
    "    for (w1,w2,w3) in allocations:\n",
    "        DS_total = w1*per_policy['P1']['DS_total'] + w2*per_policy['P2']['DS_total'] + w3*per_policy['P3']['DS_total']\n",
    "        DR_total = w1*per_policy['P1']['DR_total'] + w2*per_policy['P2']['DR_total'] + w3*per_policy['P3']['DR_total']\n",
    "        DF_total = w1*per_policy['P1']['DF_total'] + w2*per_policy['P2']['DF_total'] + w3*per_policy['P3']['DF_total']\n",
    "        rows.append({'w1':w1,'w2':w2,'w3':w3,'DS_total':DS_total,'DR_total':DR_total,'DF_total':DF_total})\n",
    "    return pd.DataFrame(rows)\n",
    "def pareto_front(df, cols):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "    for i, row in df.iterrows():\n",
    "        dominated = ((df[cols] >= row[cols]).all(axis=1)) & ((df[cols] > row[cols]).any(axis=1))\n",
    "        if dominated.any():\n",
    "            mask[i] = False\n",
    "    return mask\n",
    "def synthesize_ml2_data(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF,\n",
    "                        n_samples=500, step=0.1):\n",
    "    rng = np.random.RandomState(42)\n",
    "    records = []\n",
    "    allocs = generate_allocations(step)\n",
    "    for _ in range(n_samples):\n",
    "        context = {\n",
    "            'weather': rng.choice(['sunny','fog','stormy','cloudy']),\n",
    "            'traffic': rng.choice(['jam','medium','light']),\n",
    "            'is_peak_hour': bool(rng.choice([0,1], p=[0.7,0.3])),\n",
    "            #DS\n",
    "            'sender_district': rng.choice(['Quận 1', 'Quận 5', 'Bình Thạnh', 'Tân Bình']),\n",
    "            'receiver_district': rng.choice(['Quận 1', 'Quận 5', 'Bình Thạnh', 'Tân Bình', 'Quận 10']),\n",
    "            'shipping_distance': rng.uniform(5,50),\n",
    "            'weight': rng.uniform(500, 5000),\n",
    "            'serviceType': rng.choice(['standard', 'express']),\n",
    "            #DR\n",
    "            'Agent_Age': rng.randint(20,60),\n",
    "            'Agent_Rating': rng.uniform(3.0, 5.0),\n",
    "            'Vehicle': rng.choice(['Bike', 'Car', 'Truck']),\n",
    "            'Area': rng.choice(['Urban', 'Rural']),\n",
    "            'Order_Date': '2024-01-01',\n",
    "            'Order_Time': rng.choice(['08:00:00', '12:00:00', '18:00:00']),\n",
    "            'Pickup_Time': rng.choice(['08:30:00', '12:30:00', '18:30:00']),\n",
    "            #DF\n",
    "            'Warehouse_block': rng.choice(['A', 'B', 'C', 'D']),\n",
    "            'Mode_of_Shipment': rng.choice(['Ship', 'Flight', 'Road']),\n",
    "            'Customer_care_calls': rng.randint(1, 7),\n",
    "            'Customer_rating': rng.randint(1, 5),\n",
    "    'Cost_of_the_Product': rng.randint(100, 500),\n",
    "    'Prior_purchases': rng.randint(0, 5),\n",
    "    'Product_importance': rng.choice(['low', 'medium', 'high']),\n",
    "    'Gender': rng.choice(['M', 'F']),\n",
    "    'Discount_offered': rng.randint(0, 30),\n",
    "    'Weight_in_gms': rng.uniform(300, 4000)\n",
    "        }\n",
    "        df_alloc = compute_models_for_allocs(context, allocs, model_DS, scaler_DS, le_sender, le_receiver,\n",
    "                                             model_DR_pipeline, model_DF, encoder_DF)\n",
    "        mask = pareto_front(df_alloc, ['DS_total','DR_total','DF_total'])\n",
    "        pareto = df_alloc[mask].reset_index(drop=True)\n",
    "        # choose based on context heuristics\n",
    "        if context['weather'] in ('stormy','cloudy') or context['traffic']=='jam':\n",
    "            chosen = pareto.loc[pareto['DR_total'].idxmax()]\n",
    "        elif context['is_peak_hour']:\n",
    "            chosen = pareto.loc[pareto['DS_total'].idxmax()]\n",
    "        else:\n",
    "            chosen = pareto.loc[pareto['DF_total'].idxmax()]\n",
    "        rec = {\n",
    "            **context,\n",
    "            'w1': chosen['w1'], 'w2': chosen['w2'], 'w3': chosen['w3'],\n",
    "            'DS_total': chosen['DS_total'], 'DR_total': chosen['DR_total'], 'DF_total': chosen['DF_total']\n",
    "        }\n",
    "        records.append(rec)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def train_ml2(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF):\n",
    "    df_train = synthesize_ml2_data(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF,\n",
    "                                   n_samples=1000, step=0.1)\n",
    "    X = df_train[['weather','traffic','is_peak_hour','DS_total','DR_total','DF_total','w1','w2','w3']].copy()\n",
    "    y = df_train[['w1','w2','w3']].apply(lambda r: f\"{r.w1}_{r.w2}_{r.w3}\", axis=1)\n",
    "    X = pd.get_dummies(X, columns=['weather','traffic'], drop_first=True)\n",
    "    X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf_m2 = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf_m2.fit(X_train_m2, y_train_m2)\n",
    "    print(\"ML² trained. Accuracy:\", clf_m2.score(X_test_m2, y_test_m2))\n",
    "    return clf_m2, X.columns.tolist()\n",
    "print(\"=== TRAIN ML² ===\")\n",
    "ml2_model, ml2_features = train_ml2(model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF)\n",
    "joblib.dump(ml2_model, 'ml2_model.pkl')\n",
    "joblib.dump(ml2_features, 'ml2_features.pkl')\n",
    "print(\"Saved ML² model (ml2_model.pkl) and feature list (ml2_features.pkl)\\n\")\n",
    "\n",
    "ctx = {'weather':'sunny','traffic':'jam','is_peak_hour':True}\n",
    "allocs = generate_allocations(0.1)\n",
    "df_alloc_test = compute_models_for_allocs(ctx, allocs, model_DS, scaler_DS, le_sender, le_receiver, model_DR_pipeline, model_DF, encoder_DF)\n",
    "mask = pareto_front(df_alloc_test, ['DS_total','DR_total','DF_total'])\n",
    "pareto_df = df_alloc_test[mask].reset_index(drop = True)\n",
    "\n",
    "print(\"=== Pareto Frontier for Context ===\")\n",
    "print(ctx)\n",
    "print(pareto_df.round(3))\n",
    "\n",
    "best_speed = pareto_df.loc[pareto_df['DS_total'].idxmin()]\n",
    "best_reliab = pareto_df.loc[pareto_df['DR_total'].idxmax()]\n",
    "best_feedback = pareto_df.loc[pareto_df['DF_total'].idxmax()]\n",
    "\n",
    "print(\"\\n=== 🏆 LỰA CHỌN TỐI ƯU THEO TỪNG TIÊU CHÍ ===\")\n",
    "print(f\"- Nhanh nhất (DS thấp nhất): {dict(best_speed)}\")\n",
    "print(f\"- Đúng hạn nhất (DR cao nhất): {dict(best_reliab)}\")\n",
    "print(f\"- Hài lòng nhất (DF cao nhất): {dict(best_feedback)}\")\n",
    "\n",
    "\n",
    "# === TÍNH TRADE-OFF GIỮA CÁC LỰA CHỌN ===\n",
    "def diff(a, b):\n",
    "    return {k: round(b[k] - a[k], 3) for k in ['DS_total', 'DR_total', 'DF_total']}\n",
    "\n",
    "print(\"\\n=== ⚖️ TRADE-OFF GIỮA NHANH NHẤT vs ĐÚNG HẠN NHẤT ===\")\n",
    "print(diff(best_speed, best_reliab))\n",
    "print(\"\\n=== ⚖️ TRADE-OFF GIỮA NHANH NHẤT vs HÀI LÒNG NHẤT ===\")\n",
    "print(diff(best_speed, best_feedback))\n",
    "\n",
    "\n",
    "print(\"\\n=== ⚖️ PHƯƠNG ÁN CÂN BẰNG NHẤT (Theo Khoảng Cách Tới Điểm Lý Tưởng) ===\")\n",
    "\n",
    "# Chuẩn hóa các tiêu chí (đưa về cùng thang đo)\n",
    "df_norm = pareto_df.copy()\n",
    "df_norm['DS_norm'] = (df_norm['DS_total'] - df_norm['DS_total'].min()) / (df_norm['DS_total'].max() - df_norm['DS_total'].min())\n",
    "df_norm['DR_norm'] = (df_norm['DR_total'] - df_norm['DR_total'].min()) / (df_norm['DR_total'].max() - df_norm['DR_total'].min())\n",
    "df_norm['DF_norm'] = (df_norm['DF_total'] - df_norm['DF_total'].min()) / (df_norm['DF_total'].max() - df_norm['DF_total'].min())\n",
    "\n",
    "# Xác định hướng tối ưu:\n",
    "# - DS: càng thấp càng tốt  → giá trị lý tưởng là 0\n",
    "# - DR, DF: càng cao càng tốt → giá trị lý tưởng là 1\n",
    "ideal_point = np.array([0, 1, 1])\n",
    "# Tính khoảng cách Euclidean tới điểm lý tưởng\n",
    "df_norm['distance_to_ideal'] = np.sqrt(\n",
    "    (df_norm['DS_norm'] - ideal_point[0])**2 +\n",
    "    (df_norm['DR_norm'] - ideal_point[1])**2 +\n",
    "    (df_norm['DF_norm'] - ideal_point[2])**2\n",
    ")\n",
    "\n",
    "# Chọn phương án có khoảng cách nhỏ nhất (gần điểm lý tưởng nhất)\n",
    "best_compromise_idx = df_norm['distance_to_ideal'].idxmin()\n",
    "best_compromise = pareto_df.loc[best_compromise_idx]\n",
    "print(\"Phương án cân bằng nhất:\")\n",
    "print(best_compromise)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
